#import "simplepaper.typ": *

#show: project.with(
  title: "Attention 模块 设计报告",
  authors: (
    (
      name: "张浩宇",
      organization: [522031910129]
    ),
  ),
)

= 模块设计
== 设计目标

设计一个模块进行Self-Attention计算，输入Q、K、V矩阵，得到输出结果，并进行量化。

模块参数设为：Token数量为8，Token特征维度为4，数据采用Fix_8_8即8位整数、8位小数的定点无符号数格式存储和量化。

用python设计golden model用以验证，并用verilog完成硬件实现，要求结果仿真结果正确，代码可综合。

== 设计细节
=== Softmax简化

标准的Softmax计算公式为：$ "softmax"(A[i,j])= exp(A[i,j]-max(A[:,j]))/(sum^N_(i=1)exp(A[i,j]-max(A[:,j]))), $

实现难度较高，因此采用以下简化的Softmax计算方法：$ "softmax"(A[i,j])=(A[i,j]-min(A[:,j]))^2 $

=== 数据量化

本设计采用Fix_8_8格式的16位定点数来存储数据，即8位整数、8位小数。但在Attention计算过程中，矩阵乘法和Softmax计算会导致数据位宽增大，因此在计算过程中需要将数据量化。

具体而言，在矩阵乘法中，经过乘累加运算，输出的数据为Fix_18_16，需将其量化为Fix_8_8；在Softmax计算中，经过平方运算，输出的数据为Fix_16_16，需将其量化为Fix_8_8。

量化过程包括两种情况。在小数位数不足以表达该数值的小数部分时，本设计采用舍入的方法，即根据超出部分的大小，若小于最小精度的一半，则舍去，否则进一位。在整数部分超出位数，发生溢出时，本设计采用饱和的方法，即近似到最大的正值。

== Golden model 设计

用python设计该模块的golden model，模拟计算中的矩阵乘法、softmax和量化等过程，以验证硬件实现的正确性。为了编写方便，矩阵操作采用pytorch实现。

同时设计了python测试脚本，随机生成模块的输入数据，调用modelsim进行仿真，并将仿真结构与golden model计算结果比较，以验证硬件实现的正确性。


== 硬件模块设计
=== 实现原理

Attention 计算可以分为三个步骤，分别是：矩阵乘法Q*K#super[T]、Softmax计算、矩阵乘法Score*V。本设计采用流水线结构实现，即每一个步骤作为流水线的一级，形成3级流水，3个周期即可完成全部计算，如@fig:pipeline。

#figure(
  image("img/pipeline.svg"),
  caption: "Attention 计算的流水线结构"
)<fig:pipeline>


=== 模块总览

#figure(
  image("img/module.svg"),
  caption: "硬件模块设计"
)<fig:module>

硬件模块设计示意如@fig:module，顶层模块Attention_top输入Q、K、V矩阵，和clk、rst信号，输出计算结果。在顶层模块中，每一级流水分别作为一级子模块。在第一级子模块中，包括矩阵转置模块和矩阵乘法模块；在第二级子模块中，包括矩阵Softmax模块；在第三级子模块中，包括矩阵乘法模块。在输入和每一级流水结构中，均有寄存器，由Dff模块例化生成。

在模块设计中，各个子模块相关接口均采用参数化设计，便于模块复用与扩展。

下面介绍各模块及子模块的设计。

=== 量化模块

量化模块实现对输入数据的量化输出，在矩阵乘法模块和Softmax模块中均有使用。该模块首先判断输入输出是否溢出，即高位超出量化位数的部分是否不位0，若溢出则饱和，否则进行舍入；舍入的判断标准是超出最小精度的部分是否达到最小精度的一半，具体到实现中，通过判断低位超出量化位数部分的最高位是否为1，若为0则舍去超出部分输出，否则进位输出，若进位后发生溢出则饱和输出。

=== 矩阵乘法模块

矩阵乘法实现两个输入矩阵的乘法运算和结果的量化。该模块通过例化一系列带有量化的MAC模块实现，每个MAC模块的输入是两个输入矩阵的一行与一列，输出对应乘积矩阵的一个元素。

==== MAC（乘累加）模块

MAC模块实现两个输入向量的乘累加运算，将两个输入向量每个元素相乘后累加，并将结果通过量化模块输出。

=== 转置模块

转置模块对输入矩阵进行转置，用于Q*K#super[T]的计算中。该模块通过对输入矩阵的元素地址进行变换来实现，即将按行顺序存储的输入矩阵按列读取得到输出转置矩阵。


=== Softmax模块

Softmax模块实现对输入矩阵的Softmax计算。该模块通过例化一系列带有量化的行Softmax模块，得到每一个元素按行Softmax的后的矩阵输出。

==== 行Softmax模块

行Softmax模块实现对输入一行向量的Softmax结果。该模块具有子模块Min，通过简单的逐个对比的方式得到输入向量的最小值，然后对每一个元素计算与最小值之差的平方，并通过量化模块，得到输出的Softmax结果。

=== 寄存器模块

异步低电平有效复位的Dff模块，用于流水线中各级的寄存器。

=== Testbench

底层模块的Testbench，通过读取golden model生成的测试数据作为输入，根据测试时给定的测试次数确定仿真周期数，生成对应的时钟，每个周期将将输出结果分别以二进制和十进制浮点数的形式输出到文件中，以供后续对比验证。


= 设计结果
== 计算结果

根据测试脚本的结果，在多次随机输入的测试中，模块输出结果均与golden model计算结果一致，验证了硬件实现的正确性。

由于对Softmax计算进行了简化，Softmax的结果并未归一化到0\~1之间，较大的值经过Softmax后会被变得更大，容易导致大量结果溢出饱和，可令输入数据较小即仅使用部分的输入数据位宽来生成输入矩阵，以减少溢出的概率，从而提高测试结果的可读性。

== 时序

#figure(
  image("img/seq.png"),
  caption: "时序仿真结果"
)<fig:timing>

在一次进行五组输入测试的仿真过程中，时序仿真结果如@fig:timing 所示。可以看到，数据输入三个时钟周期后，产生对应的输出结果，从开始输入数据后，经过三个周期的启动延迟，得到第一个输入对应的输出，此后每一个周期输出对应三个周期前输入的结果。可见流水线结构时序正确，符合设计。

== 综合

#figure(
   grid(
    columns: 1,
    rows: 4,
    row-gutter: 2mm,
    column-gutter: 4mm,
    image("img/schematic_analysis.svg"), 
    [(a)Analysis 原理图],
    image("img/schematic.jpg",width: 100%), 
    [(b)Synthesis 原理图]
  ), 

  caption: "综合结果"
)

= 讨论
== 数据流分析
=== 关键路径

结合模块设计和综合结果

=== 延迟与吞吐率



=== 折叠设计

如果用折叠的方式实现该模块

== 量化中的舍入
